{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use COCO to test our image captioning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Implement the dataloader for COCO. This is used to train the image captioning model.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.captions = COCO('./data/Coco/annotations/captions_val2017.json')\n",
    "        self.num_tokens = 15000\n",
    "\n",
    "        #Keep things very simple for now - just have these 4 special tokens\n",
    "        #Change later to prevent initialization from messing up these tokens - save to text file\n",
    "        #somewhere instead of initializing inside dataset\n",
    "\n",
    "        self.eos = np.random.randn(50) #index 0\n",
    "        self.bos = np.random.randn(50) #index 1\n",
    "        self.unk = np.random.randn(50) #index 2\n",
    "        self.pad = np.random.randn(50) #index 3\n",
    "        glove_data_filepath = './data/glove.6B.50d.txt'\n",
    "\n",
    "        #I wanted to try something a bit different from the paper - that is, using pretrained word embeddings\n",
    "        #instead. This fits in with the spirit of the paper in learning with less information\n",
    "        #In practice, we probably want something like nltk's tokenize, or implement a scheme similar to\n",
    "        #BERT's wordpiece tokenizer. Here as the data is already fairly clean (and to prevent me from\n",
    "        #spending a ton of time tinkering with the tokenizer), we simply split on spaces and convert\n",
    "        #to lowercase. OOV words are replaced by the UNK token. If you want to get really fancy, encode\n",
    "        #words using something like BERT's encoding layer.\n",
    "\n",
    "        df = pd.read_csv(glove_data_filepath, sep=\" \", quoting=3, header=None, index_col=0).head(self.num_tokens)\n",
    "\n",
    "        words = list(df.T.items())\n",
    "        self.word_vector_dict = {key: (i+4, val.values) for i, (key, val) in enumerate(words)}\n",
    "\n",
    "        self.max_length = 16\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        caption = self.captions.dataset['annotations'][idx]['caption']\n",
    "        words = caption[:-1].split(' ') + [caption[-1]]\n",
    "        word_indices = [1]\n",
    "        caption_embeddings = [self.bos]\n",
    "        for word in words[:self.max_length]:\n",
    "\n",
    "            try:\n",
    "\n",
    "                index, word_vector = self.word_vector_dict[word.lower()]\n",
    "                caption_embeddings.append(word_vector)\n",
    "                word_indices.append(index)\n",
    "            except KeyError:\n",
    "                caption_embeddings.append(self.unk)\n",
    "                word_indices.append(2)\n",
    "\n",
    "        len_caption = len(caption_embeddings)\n",
    "        len_padding = self.max_length + 1 - len_caption\n",
    "\n",
    "        caption_embeddings = caption_embeddings + [self.eos] + [self.pad]*len_padding\n",
    "        word_indices = word_indices + [0] + [3]*len_padding\n",
    "\n",
    "        picture_name = str(self.captions.dataset['annotations'][idx]['image_id']).zfill(12)\n",
    "        picture_filepath = f'./data/Coco/images/{picture_name}.jpg'\n",
    "\n",
    "        #In a real life scenario, I would probably normalize these images using global mean/variance\n",
    "        #statistics as well as add in augmentation. Also the images are very small, but here I just wanted\n",
    "        #to test that the model works on my crappy GPU. This is very proof of concept.\n",
    "        picture = cv2.imread(picture_filepath)\n",
    "        picture = cv2.resize(picture, (28, 28))\n",
    "        picture = cv2.cvtColor(picture, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        return picture/255, np.array(caption_embeddings), np.array(word_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.captions.dataset['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=128):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=128, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels,4, kernel_size=5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, kernel_size=5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, image_channels, kernel_size=6, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar, num_samples):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "\n",
    "        if num_samples == 1:\n",
    "            esp = torch.randn(*mu.size())\n",
    "        else:\n",
    "            esp = torch.randn(num_samples, *mu.size())\n",
    "\n",
    "        z = mu + std * esp.to(device)\n",
    "        return z\n",
    "\n",
    "    def bottleneck(self, h, num_samples):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar, num_samples)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x, num_samples=1):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h, num_samples=num_samples)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        num_tokens = 15000\n",
    "        glove_embedding_dim = 50\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(glove_embedding_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, num_tokens)\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_length = 16 + 2\n",
    "\n",
    "    def get_last_token(self, x, hidden):\n",
    "\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        outputs = torch.zeros(self.max_length, x.shape[0], self.num_tokens)\n",
    "        input = x[:,0,:].unsqueeze(1)\n",
    "\n",
    "        for i in range(1, self.max_length):\n",
    "\n",
    "            output, hidden = self.get_last_token(input, hidden)\n",
    "            outputs[i,...] = output.squeeze(1)\n",
    "\n",
    "            input = x[:,i,:].unsqueeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred_x, x, mu, logvar, pred_caption=None, caption=None):\n",
    "\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\n",
    "    BCE = F.binary_cross_entropy(pred_x, x, size_average=False)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    if caption != None:\n",
    "        sentence_loss = F.cross_entropy(pred_caption, word_indices.long(), ignore_index=2)\n",
    "        return alpha*(BCE + KLD) + beta*sentence_loss\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    #Let's use coco\n",
    "\n",
    "    coco = CocoDataset()\n",
    "    batch_size = 64\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    vae = VAE(image_channels=1).to(device)\n",
    "    rnn = RNN().to(device)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_samples = 10\n",
    "    num_tokens = 15000\n",
    "\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, (images, caption, word_indices) in enumerate(dataloader):\n",
    "\n",
    "            images = images.float().unsqueeze(1).to(device)\n",
    "            caption = caption.float().to(device)\n",
    "            \n",
    "            \n",
    "            #Get ten samples from each distribution\n",
    "            #Variance reduction from reparametrization trick\n",
    "\n",
    "            monte_carlo_embeddings, mu, logvar = vae.encode(images, num_samples=num_samples)\n",
    "            words_per_sentence = torch.count_nonzero(word_indices - 3, axis=1)\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for embedding in monte_carlo_embeddings:\n",
    "\n",
    "                #Using teacher forcing, get predictions for each word in the caption\n",
    "                pred_caption = rnn(caption, embedding.view(1, batch_size, -1))\n",
    "                pred_caption = pred_caption.permute(1,2,0)\n",
    "\n",
    "                pred_images = vae.decode(embedding)\n",
    "                loss += loss_fn(pred_images, images, mu, logvar, pred_caption, caption)\n",
    "\n",
    "            loss /= num_samples\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
