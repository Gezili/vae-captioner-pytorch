{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports, global information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use COCO to test our image captioning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Implement the dataloader for COCO. This is used to train the image captioning model.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.captions = COCO('./data/Coco/annotations/captions_val2017.json')\n",
    "        self.num_tokens = 15000\n",
    "\n",
    "        #Keep things very simple for now - just have these 4 special tokens\n",
    "        #Change later to prevent initialization from messing up these tokens - save to text file\n",
    "        #somewhere instead of initializing inside dataset\n",
    "\n",
    "        self.eos = np.random.randn(50) #index 0\n",
    "        self.bos = np.random.randn(50) #index 1\n",
    "        self.unk = np.random.randn(50) #index 2\n",
    "        self.pad = np.random.randn(50) #index 3\n",
    "        glove_data_filepath = './data/glove.6B.50d.txt'\n",
    "\n",
    "        #I wanted to try something a bit different from the paper - that is, using pretrained word embeddings\n",
    "        #instead. This fits in with the spirit of the paper in learning with less information\n",
    "        #In practice, we probably want something like nltk's tokenize, or implement a scheme similar to\n",
    "        #BERT's wordpiece tokenizer. Here as the data is already fairly clean (and to prevent me from\n",
    "        #spending a ton of time tinkering with the tokenizer), we simply split on spaces and convert\n",
    "        #to lowercase. OOV words are replaced by the UNK token. If you want to get really fancy, encode\n",
    "        #words using something like BERT's encoding layer.\n",
    "\n",
    "        df = pd.read_csv(glove_data_filepath, sep=\" \", quoting=3, header=None, index_col=0).head(self.num_tokens)\n",
    "\n",
    "        words = list(df.T.items())\n",
    "        self.word_vector_dict = {key: (i+4, val.values) for i, (key, val) in enumerate(words)}\n",
    "\n",
    "        self.max_length = 16\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        caption = self.captions.dataset['annotations'][idx]['caption']\n",
    "        words = caption[:-1].split(' ') + [caption[-1]]\n",
    "        word_indices = [1]\n",
    "        caption_embeddings = [self.bos]\n",
    "        for word in words[:self.max_length]:\n",
    "\n",
    "            try:\n",
    "\n",
    "                index, word_vector = self.word_vector_dict[word.lower()]\n",
    "                caption_embeddings.append(word_vector)\n",
    "                word_indices.append(index)\n",
    "            except KeyError:\n",
    "                caption_embeddings.append(self.unk)\n",
    "                word_indices.append(2)\n",
    "\n",
    "        len_caption = len(caption_embeddings)\n",
    "        len_padding = self.max_length + 1 - len_caption\n",
    "\n",
    "        caption_embeddings = caption_embeddings + [self.eos] + [self.pad]*len_padding\n",
    "        word_indices = word_indices + [0] + [3]*len_padding\n",
    "\n",
    "        picture_name = str(self.captions.dataset['annotations'][idx]['image_id']).zfill(12)\n",
    "        picture_filepath = f'./data/Coco/images/{picture_name}.jpg'\n",
    "\n",
    "        #In a real life scenario, I would probably normalize these images using global mean/variance\n",
    "        #statistics as well as add in augmentation. Also the images are very small, but here I just wanted\n",
    "        #to test that the model works on my crappy GPU. This is very proof of concept.\n",
    "        picture = cv2.imread(picture_filepath)\n",
    "        picture = cv2.resize(picture, (28, 28))\n",
    "        picture = cv2.cvtColor(picture, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        return picture/255, np.array(caption_embeddings), np.array(word_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.captions.dataset['annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=128):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the VAE. This is responsible for generating the latent space. Modified from https://github.com/sksq96/pytorch-vae, with modified encoder and decoder architectures as well as implemented monte carlo sampling of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=128, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels,4, kernel_size=5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 8, kernel_size=5),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, image_channels, kernel_size=6, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar, num_samples):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "\n",
    "        if num_samples == 1:\n",
    "            esp = torch.randn(*mu.size())\n",
    "        else:\n",
    "            esp = torch.randn(num_samples, *mu.size())\n",
    "\n",
    "        z = mu + std * esp.to(device)\n",
    "        return z\n",
    "\n",
    "    def bottleneck(self, h, num_samples):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar, num_samples)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x, num_samples=1):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h, num_samples=num_samples)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN for training on captions. Implements an autogenerative GRU with teacher forcing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        num_tokens = 15000\n",
    "        glove_embedding_dim = 50\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(glove_embedding_dim, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, num_tokens)\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_length = 16 + 2\n",
    "\n",
    "    def get_last_token(self, x, hidden):\n",
    "\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "        return prediction, hidden\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        outputs = torch.zeros(self.max_length, x.shape[0], self.num_tokens)\n",
    "        input = x[:,0,:].unsqueeze(1)\n",
    "\n",
    "        for i in range(1, self.max_length):\n",
    "\n",
    "            output, hidden = self.get_last_token(input, hidden)\n",
    "            outputs[i,...] = output.squeeze(1)\n",
    "\n",
    "            input = x[:,i,:].unsqueeze(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function. Integrates KL divergence and BCE for images with Softmax from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred_x, x, mu, logvar, pred_caption=None, caption=None):\n",
    "\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\n",
    "    BCE = F.binary_cross_entropy(pred_x, x, size_average=False)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    if caption != None:\n",
    "        sentence_loss = F.cross_entropy(pred_caption, caption.long(), ignore_index=2)\n",
    "        return alpha*(BCE + KLD) + beta*sentence_loss\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training script. As we are not actually running training, no attempt was made at tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    #Let's use coco\n",
    "\n",
    "    coco = CocoDataset()\n",
    "    batch_size = 64\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    vae = VAE(image_channels=1).to(device)\n",
    "    rnn = RNN().to(device)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_samples = 10\n",
    "    num_tokens = 15000\n",
    "\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, (images, caption, word_indices) in enumerate(dataloader):\n",
    "\n",
    "            images = images.float().unsqueeze(1).to(device)\n",
    "            caption = caption.float().to(device)\n",
    "            \n",
    "            \n",
    "            #Get ten samples from each distribution\n",
    "            #Variance reduction from reparametrization trick\n",
    "\n",
    "            monte_carlo_embeddings, mu, logvar = vae.encode(images, num_samples=num_samples)\n",
    "            loss = 0\n",
    "\n",
    "            for embedding in monte_carlo_embeddings:\n",
    "\n",
    "                #Using teacher forcing, get predictions for each word in the caption\n",
    "                pred_caption = rnn(caption, embedding.view(1, batch_size, -1))\n",
    "                pred_caption = pred_caption.permute(1,2,0)\n",
    "\n",
    "                pred_images = vae.decode(embedding)\n",
    "                \n",
    "                #Approximate expected value of loss function with monte carlo sampling\n",
    "                loss += loss_fn(pred_images, images, mu, logvar, pred_caption, caption)\n",
    "\n",
    "            loss /= num_samples\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Loss is 34953.45703125\n",
      "Loss is 34943.2265625\n",
      "Loss is 34899.25\n",
      "Loss is 34877.1953125\n",
      "Loss is 34871.3984375\n",
      "Loss is 34831.62109375\n",
      "Loss is 34831.01953125\n",
      "Loss is 34820.69921875\n",
      "Loss is 34806.29296875\n",
      "Loss is 34785.96484375\n",
      "Loss is 34767.53125\n",
      "Loss is 34761.69921875\n",
      "Loss is 34736.5625\n",
      "Loss is 34682.5234375\n",
      "Loss is 34763.80859375\n",
      "Loss is 34657.05859375\n",
      "Loss is 34569.20703125\n",
      "Loss is 34621.66796875\n",
      "Loss is 34658.13671875\n",
      "Loss is 34655.40234375\n",
      "Loss is 34646.80078125\n",
      "Loss is 34581.54296875\n",
      "Loss is 34551.6328125\n",
      "Loss is 34455.87109375\n",
      "Loss is 34416.40234375\n",
      "Loss is 34526.41796875\n",
      "Loss is 34465.3671875\n",
      "Loss is 34675.38671875\n",
      "Loss is 34475.8515625\n",
      "Loss is 34497.85546875\n",
      "Loss is 34473.85546875\n",
      "Loss is 34535.40234375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-7013725fcb08>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
